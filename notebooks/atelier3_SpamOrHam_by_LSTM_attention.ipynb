{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "introLstmAttention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMoPAdYhleyJdxjs40Vbvf3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTuuSwd9xG4_"
      },
      "source": [
        "# RNN, LSTM et Attention : traiter des données séquentielles\n",
        "\n",
        "Nous allons voir ce qu'est un réseau de neurones récurrent, en quoi il est utile et quelles sont les applications possibles.\n",
        "Face à un GROS problème des RNN, nous verrons un autre type de RNN moins naif : le Long-Short Term Memory. Enfin, face à encore une autre limitation des LSTM, nous verrons une dernière amélioration qui ouvre la porte à l'état de l'art : le mécanisme d'attention.\n",
        "\n",
        "On arrête pas le progrès."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdzCjRdSx_K-"
      },
      "source": [
        "##1. RNN- LSTM : théorie et applications\n",
        "\n",
        "Jusqu'à present, nous avons vu comment traiter des donnees qui se comportent bien :\n",
        "  - Des tableaux de chiffres\n",
        "  - Des variables catégorielles pouvant être discretisées\n",
        "  - Des images, qui sont de belles matrices de chiffres\n",
        "\n",
        "Mais comment faire avec des données un peu plus punk ?\n",
        "ex: les séries temporelles, des enregistrements audio ou des textes de journaux.\n",
        "\n",
        "Le point commun de tous ces types de données est qu'on trouve des durées variables à l'intérieur d'un même dataset: des phrases plus ou moins longues, des enregitrements de quelques secondes vs quelques minutes, etc. De plus, l'information à date $t$ dépend souvent de l'information à $t-1$: le mot d'après dépend du mot d'avant dans une phrase, le prix d'une action dépend de son prix il y a trois heures. A cette fin, on utilise un réseau de neurones récurrent, ou RNN.\n",
        "\n",
        "###a. RNN \"vanilla\"\n",
        "\n",
        "L'idée du RNN a été proposée en 1986, quand les auteurs de [1] ont réussi à réaliser la backpropagation d'un tel réseau. De nombreuses variantes existent, mais nous présentons ici un RNN relativement agnostique à toutes ces petites variations.\n",
        "\n",
        "![rnn unfolded](https://drive.google.com/uc?id=1cJjqpuBYDjFdcq-voRzfsRslWUuftNlC)\n",
        "\n",
        "Un RNN est donc un réseau de neurones qui prend en compte des données séquentielles. Le but est de prendre en compte le passé pour donner du sens au présent, et de prendre en compte tous les mots d'une phrase par exemple, dans la représentation finale.\n",
        "\n",
        "Soit une séquence de données $x$=$x_1, x_2, ...x_T$. On va entrainer le même réseau $T$ fois, soit autant que de mots dans la phrase. Chaque input sera un mélange de nouvelle information (un nouveau $x_t$) et de la connaissance passée (stockée dans l'output précédent $h_t$)\n",
        "\n",
        "Pour les états cachés, on a donc les équations suivantes (en omettant les biais) pour la passe en avant:\n",
        "\\begin{eqnarray}\n",
        "h_t = \\theta \\left( W \\cdot \\left[ x_t, h_{t-1}\\right] \\right)\n",
        "& =\\theta \\left( W_X x_t + W_H h_{t-1}\\right)\n",
        "\\end{eqnarray}\n",
        "où\n",
        "- $\\theta$ est $tanh$, ou n'importe quelle fonction d'activation\n",
        "- W une matrice de poids\n",
        "- $x_t$ l'input à date t\n",
        "- $h_{t-1}$ l'état précédent du RNN\n",
        "\n",
        "\\begin{equation}\n",
        "y_t = F(W_Yh_t)\n",
        "\\end{equation}\n",
        "où\n",
        "- $y_t$ est l'output du réseau pour une date $t$\n",
        "- $W_Y$ est une matrice de poids\n",
        "- $F()$ est une fonction d'activation, originellement juste l'identité\n",
        "- $h_t$ est l'état caché du réseau pour une date $t$\n",
        " \n",
        "\n",
        "<!---\n",
        "On peut réécrire ces équations plus formellement avec :\n",
        "$h_t = \\begin{pmatrix} h_{t1} & h_{t2} & \\dots & h_{tH} \\end{pmatrix} ^\\intercal$,  $y_t = \\begin{pmatrix} y_{t1} & y_{t2} & \\dots & y_{tN} \\end{pmatrix}^\\intercal$ et $x_t = \\begin{pmatrix} x_{t1} & x_{t2} & \\dots & x_{tI} \\end{pmatrix}^\\intercal$\n",
        "\n",
        "et en passant la notation de temps en exposant pour plus de lisibilité :\n",
        "$h_{k}^t = \\sum_{i=1}^I w_{ik}x_i^t + \\sum_{d=1}^H w_{dk}b_{d}^{t-1}$\n",
        "\n",
        "$b_h^t = \\theta_h \\left( h_k^t \\right)$\n",
        "\n",
        "$y_h^t = \\sum_{h'=1}^H w_{h'k}b_h^t$\n",
        "--->\n",
        "\n",
        "###b.LSTM\n",
        "\n",
        "Un RNN basique prend donc en compte ce qui se passe l'étape d'avant pour décider de l'étape d'après. Mais souvent, on a besoin d'apprendre plus loin que l'étape précédente, et de prendre en compte tout le passé, ou du moins le plus pertinent. \n",
        "\n",
        "<!---\n",
        "Si vous voyez un individu s'avancer vers vous avec un couteau, vous allez surement courir. Sauf si vous vous souvenez que c'est votre ami Jérémy qui vous rend le couteau de votre grand-père que vous lui aviez prété.\n",
        "Mais le fait que c'est le couteau de votre grand-père a peu d'importance comparé au fait que vous avez convaincu son-sa fiancé-e de convoler avec vous la veille de leur mariage. Tout l'enjeu d'un LSTM est de se souvenir des faits les plus saillants pour obtenir les meilleures représentations des situations.\n",
        "-->\n",
        "\n",
        "Un réseau Long-Short Term Memory est un RNN avec des portes, qui gèrent l'oubli des valeurs précédentes. Dans sa version la plus courante, celle de [2], une cellule LSTM possède trois portes, que nous allons décomposer.\n",
        "\n",
        "![lstm cell](https://drive.google.com/uc?id=1LfOd1ZkJf25tEQGo8AYqADYoYm_VNyt4)\n",
        "![lstm cell](https://drive.google.com/uc?id=1Ho4pj0ybUHAUzAlyEssuF8VXc7II6vqp)\n",
        "\n",
        "Premièrement, une \"cellule\" LSTM est comme une \"cellule\" de RNN, on lui passe un état caché et un input $x_t$ et il calcule des choses. L'état interne n'est plus l'état caché ni un simple produit suivi d'une multiplication, mais il reste simple : c'est la ligne du haut dans la cellule, $C_t^l$\n",
        "\n",
        "Les trois portes sont trois sigmoides, qui donnent des valeurs entre 0 et 1, 0 signifiant \"on laisse rien passer\"/\"on oublie tout\" et 1 signifiant \"on laisse passer toute l'information\".\n",
        "\n",
        "1. La première porte, $f_t$, s'appelle \"forget gate layer\" (la porte de l'oubli). Elle regarde l'état caché précédent, la nouvelle valeur, et décide de garder ou pas l'ancienne mémoire.\n",
        "$$f_t = \\sigma\\left( W \\cdot \\left[ x_t, h_{t-1}\\right] + b_f \\right)$$\n",
        "Cette équation ne vous rapelle pas quelque chose ?\n",
        "\n",
        "2. La deuxième porte $i_t$ est l'\"input gate layer\"(la porte de l'entrée/la contribution). Elle décide quelles valeurs vont être modifiées/mises à jour.\n",
        "En parallèle, une autre couche tanh crée de nouvelles valeurs candidates: quelles seraient les valeurs si on pouvait toutes les mettre à jour. Quand on combine les deux, cela nous donne les nouvelles valeurs à garder en mémoire.\n",
        "$$i_t = \\sigma \\left( W_i \\cdot [x_t, h_{t-1}] +b_i \\right)$$\n",
        "$$\\tilde{C}_t = tanh \\left( W_C \\cdot[x_t, h_{t-1}] +b_C \\right)$$\n",
        "\n",
        "et donc :\n",
        "$$\\hat{C}_t = i_t * \\tilde{C}_t $$\n",
        "\n",
        "Ces deux portes apportent des modifications à l'état interne de la cellule, en décidant d'à quel point on retient le passé, et comment modifier l'état interne de la cellulle pour prendre en compte les nouvelles valeurs.\n",
        "\n",
        "$$C_t = f_t * C_{t-1} + \\hat{C}_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$$\n",
        "\n",
        "3. La dernière porte est l'\"output gate\" (la porte de sortie). Elle ne touche plus à la mémoire de la cellule, toutes les modifications ont déjà été faites. Elle se contente de filtrer ce qu'il y a en mémoire pour donner une sortie. On prend l'état courant, que l'on fait passer par tanh (pour normaliser). En parallèle, une couche sigmoide décide de quelles valeurs on va donner en sortie. Comme pour l'input, la multiplication des 2 donne les valeurs de l'ouput.\n",
        "\n",
        "$$o_t = \\sigma \\left( W_o [x_t, h_{t-1}] +b_o \\right)$$\n",
        "$$h_t = o_t * tanh \\left( C_t \\right)$$\n",
        "\n",
        "---\n",
        "<!---\n",
        "Un exemple filé : prédire un mot plausible après dans la phrase \"Mon frère est avocat, ma soeur est _____\".\n",
        "- La première porte de l'oubli permet par exemple d'oublier l'information sur le genre: nous avions un genre masculin, mais le fait d'avoir vu un nouveau sujet féminin, 'soeur', nous indique que maintenant le sujet est féminin. Quand on voit un nouveau sujet, on veut oublier le genre de l'ancien.\n",
        "- La deuxième porte de l'entrée, elle, va permettre de mettre dans l'état interne (ie, en mémoire), que le sujet courant est de genre féminin.\n",
        "- La porte de sortie, va elle, voir que nous avions un verbe, elle va donc surement donner de l'information concernant un nom, car souvent un nom vient apres un verbe. Il va donc donner le genre et le nombre du sujet, la conjugaison du verbe, pour accorder le nom qui vient.\n",
        "\n",
        "Les LSTM ont été un énorme pas dans ce que l'on peut faire avec les RNN. Y a-t-il eu d'autres grands pas derrière ? Oui, il y a un consensus pour dire que le grand pas d'après a été l'attention. Ce n'est pas le seul, mais il est à la base d'un modèle qui fut état-de-l'art jusqu'en juillet 2020.\n",
        "--->\n",
        "\n",
        "\n",
        "###c. Méchanisme d'attention\n",
        "\n",
        "####1. Modèle seq2seq : motivation pour l'attention\n",
        "\n",
        "Historiquement, seq2seq est un modèle qui permet de faire de la traduction: on donne une séquence, le modèle sort une nouvelle séquence.\n",
        "\n",
        "La mécanique basique est consitué de deux LSTM l'un après l'autre : le premier, appelé encodeur, encode toute la séquence dans le dernier état caché $h_T$. \n",
        "Le deuxième LSTM, le décodeur, prend en entrée cet encoding de la séquence et le premier mot (un token \\<START\\>), et sort un premier output. A chaque étape, il prend l'état caché $h'_t$ précédent et l'output qu'il vient de faire (le dernier mot prédit) pour faire une prédiction. On continue comme ça jusqu'à prédire un token de fin (\\<END\\>)\n",
        "\n",
        "![lstm cell](https://drive.google.com/uc?id=13fmMDB3on4a9ZYnuYSCqpORBsxB-ESyb)\n",
        "\n",
        "Le problème se trouve dans l'état du milieu, le fameux $c$, qui doit contenir toute l'information de la phrase. En réalité, si ce dernier output d'un LSTM est souvent suffisant pour encoder un thème, un sentiment, un topic général de la phrase, il n'est pas suffisant pour retenir toute l'information nécéssaire à une traduction. Il lui manque un élément crucial, le contexte.\n",
        "\n",
        "####2.Attention générale\n",
        "\n",
        "C'est là qu'apparait l'attention.\n",
        "L'attention, de façon très générale, est une couche d'un réseau de neurones qui est en charge de quantifier les interdependences entre les inputs/outputs (attention générale) ou entre les inputs eux-mêmes (self-attention). Nous parlerons ici d'attention générale.\n",
        "\n",
        "![attention mecanism](https://drive.google.com/uc?id=1uXzpMr_-G-WAhTrJf7VM4qzo2dLhTmqs)\n",
        "\n",
        "Notation : soient $h_1, h_2..., h_T$ les états cachés du décodeur, et\n",
        "$\\bar{h}_1, \\bar{h}_2, ..., \\bar{h}_S$ les états cachés de l'encodeur. \n",
        "\n",
        "Le premier morceau, l'encodage, reste le même, un LSTM classique avec ses inputs, ses états cachés $h_t$.\n",
        "\n",
        "On crée un LSTM pour le décodage. On devrait , sans attention, passer dans ce LSTM, la dernière prédiction $y_t$ et uniquement le dernier état caché $\\bar{h}_t$.\n",
        "Pour l'attention, on concatène au dernier état caché $\\bar{h}_t$ un vecteur de contexte qui montre à quel point le dernier état caché du décodeur s'aligne avec la phrase d'origine.\n",
        "\n",
        "\n",
        "On choisit d'abord un score d'alignement entre les $h$ et les $\\bar{h}$. Le choix de ce score est ce qui fait la différence entre l'attention Bahdanau (ou additive) et Luong (ou multiplicative).\n",
        "\n",
        "\\begin{equation}\n",
        "  score(h_t, \\bar{h}_s) = \\begin{cases}\n",
        "      h_t^\\intercal W \\bar{h}_s & \\text{Pour Luong/multiplicative}\\\\\n",
        "      v_a^\\intercal tanh(W_1h_t + W_2 \\bar{h}_s) & \\text{Pour Bahdanau/additive}\n",
        "    \\end{cases}       \n",
        "\\end{equation}\n",
        "\n",
        "Puis on crée les poids d'attention. On utilise softmax pour rapporter le score à une distribution de probabilité (la somme de tous les poids doit faire 1).\n",
        "\n",
        "$$ \\alpha_{ts} = \\frac{exp(score(h_t, \\bar{h}_s))}{\\sum_{s'=1}^{S} exp(score(h_t, \\bar{h}_{s'}))} $$\n",
        "\n",
        "\n",
        "On pondère chaque état caché de l'encodeur (donc, finalement, chaque \"encoding\" de chaque mot d'origine) par ces poids d'attention, et on somme. Cela donne un vecteur, issu d'une somme des mots pondérée par combien l'état caché du décodeur \"colle\" à chaque mot de départ.\n",
        "\n",
        "$$c_t = \\sum_{s} \\alpha_{ts}\\bar{h}_s $$\n",
        "\n",
        "Finalement, on concatène ce vecteur de contexte à l'état courant du décodeur &h_t&. Cela donne un \"nouveau\" $h_t$, $a_t$, plus complet, qui prend en compte la coordination entre l'état courant et les mots passés dans l'encodeur. C'est sur ce vecteur que l'on ajoute une dernière couche entièrement connectée (fully connected) pour faire la prédiction d'un mot.\n",
        "\n",
        "$$a_t = tanh(W_c[c_t;h_t])$$\n",
        "\n",
        "Et on l'utilise comme si c'était notre état caché $h_t$ !\n",
        "\n",
        "---\n",
        "\n",
        "## Sources des images et références\n",
        "\n",
        "Source de l'image RNN + détails:\n",
        "https://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "\n",
        "Source de l'image LSTN + détails :\n",
        "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n",
        "\n",
        "\n",
        "Pour plus de détails sur l'attention :\n",
        "https://machinelearningmastery.com/attention-long-short-term-memory-recurrent-neural-networks/\n",
        "https://blog.floydhub.com/attention-mechanism/\n",
        "\n",
        "[1] Rumelhart, D., Hinton, G. & Williams, R. Learning representations by back-propagating errors. Nature 323, 533–536 (1986). https://doi.org/10.1038/323533a0\n",
        "\n",
        "[2] Hochreiter, Sepp & Schmidhuber, Jürgen. (1997). Long Short-term Memory. Neural computation. 9. 1735-80. 10.1162/neco.1997.9.8.1735. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5pHxTrO9Gs7"
      },
      "source": [
        "##2. Données de travail : spam or ham ?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmmGMFRo9OPr"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import text\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.layers import Input, SimpleRNN, Dense, Embedding, LSTM, Masking, Concatenate\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from keras.callbacks import LambdaCallback, History, EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJnO75oidUtD"
      },
      "source": [
        "On importe les données, et on regarde rapidement leur distribution, quelques exemples pour comprendre leur format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbVOY6MA9YX-"
      },
      "source": [
        "#Import the data, check it\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/CMallart/ateliers-NN/main/data/spamorham/SPAM%20text%20message%2020170820%20-%20Data.csv\")\n",
        "data.head()\n",
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jc-EN8gC8ir"
      },
      "source": [
        "cnt_pro = data['Category'].value_counts()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.bar(cnt_pro.index, cnt_pro.values, alpha=0.8, color=['blue', 'orange'])\n",
        "plt.ylabel('Number of Occurrences', fontsize=12)\n",
        "plt.xlabel('Category', fontsize=12)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x510A-edjyd"
      },
      "source": [
        "Plus précisément, puisqu'on traite du language, on peut regarder quels types de phrases apparaissent dans la base. Cela sert à se donner une idée de la complexité à donner au modèle : le language est-il complexe ? A l'oeil nu, pouvons-nous faire la tache de classification ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puVXsg0097Ly"
      },
      "source": [
        "# What kind of language is used, what kind of cleanup is necessary\n",
        "def see_message(ind):\n",
        "  mess=data.iloc[ind][\"Message\"]\n",
        "  print(mess)\n",
        "  return(mess)\n",
        "for message in [0,15,25,35,50]:\n",
        "  see_message(message)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjdPXhs4d-YM"
      },
      "source": [
        "Puisqu'on va traiter le texte et uniquement le texte, s'il existe dans les données quelques exemples sans texte, on les retire."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXLK5z3S5Sbm"
      },
      "source": [
        "#check if we have empty strings, or punctuation only columns\n",
        "#they are not important here, we remove them\n",
        "to_remove =[]\n",
        "for i, row in data.iterrows():\n",
        "  if re.match(r'.*[A-Za-z]+.*',row[\"Message\"]) is None :\n",
        "    print(\"'\",row[\"Message\"],\"', at line\", i)\n",
        "    to_remove.append(i)\n",
        "data = data.drop(to_remove, axis =0) \n",
        "print(\"Removed the punctuation or numbers-only lines\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcH1xgAmeb1V"
      },
      "source": [
        "Enfin, on nettoie le texte, pour le débarasser notamment des adresses web. Cela permet de se concentrer uniquement sur le texte, et de ne pas avoir un modèle qui apprenne trop vite qu'un message avec une adresse web est forcément indésirable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wdmjf3soBTL5"
      },
      "source": [
        "#A little cleanup is necessary \n",
        "#I won't bother cleaning up the puctuation as it is done later by keras.preprocessing.tokenizer\n",
        "def clean_text(text):\n",
        "  #remove the https:// stuff\n",
        "  text = re.sub(r\"http\\S+\", \"\", text)\n",
        "  #remove the urls that don't have the http:// => solution = remove all strings that have a [some non-space characters].[two or three letters]/[some non-space characters] or [some non-space characters].[two or three letters]?[some non-space characters]\n",
        "  text = re.sub(r\"[^\\s]*\\.[a-z]{2,3}(\\\\|\\?)*[^\\s]*\", \"\", text)\n",
        "  return(text)\n",
        "\n",
        "clean_text(see_message(15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yuj_vg42e_Ps"
      },
      "source": [
        "On commence à mettre en forme les données pour entrer dans le réseau de neurones.\n",
        "\n",
        "**Note:** l'id 0 est par défaut réservé et jamais assigné."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys3PVm2HEPwo"
      },
      "source": [
        "def tokenize_and_encode(text_data):\n",
        "  #fit Tokenizer : from sequences of words to lists of numbers\n",
        "  tokenizer = text.Tokenizer(\n",
        "            filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "            lower=True,\n",
        "            split=\" \",\n",
        "            oov_token=\"OUT_OF_VOCABULARY\"\n",
        "        )\n",
        "  tokenizer.fit_on_texts(text_data)\n",
        "  sequences = tokenizer.texts_to_sequences(text_data)\n",
        "  print(\"Example of output : \",  sequences[2])\n",
        "  print(\" For the input : \",text_data[2])\n",
        "\n",
        "  #pad the data for input into the RNN \n",
        "  #maxlen = max([len(x) for x in sequences])\n",
        "  padded_sequences = sequence.pad_sequences(sequences, padding=\"post\", maxlen=50)\n",
        "\n",
        "  print('Number of Unique Tokens: %d' % len(tokenizer.word_index))\n",
        "  return(padded_sequences, len(tokenizer.word_index)+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG7UnHYwgJTN"
      },
      "source": [
        "On encode les données et les targets. Les targets sont one-hot-encodées."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dx0F5NEHl9N"
      },
      "source": [
        "#encoding the whole dataset\n",
        "\n",
        "#targets\n",
        "#Binarize categories\n",
        "targets = data[\"Category\"].apply(lambda x: 0 if x==\"spam\" else 1).tolist()\n",
        "one_hot_targets = np.eye(2)[targets]\n",
        "\n",
        "#text\n",
        "text_data, vocab_size = tokenize_and_encode(data[\"Message\"].apply(clean_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi1AcAYCgY57"
      },
      "source": [
        "On sépare la base en base d'entrainement et base de test, et on vérifie que tout est au bon format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "330prtBWLmFI"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(text_data, one_hot_targets, test_size=0.33, random_state=17)\n",
        "\n",
        "#Convert those arrays to numpy as it is the desired input to the Keras RNN\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(\"Example of encoded train sequence: \", X_train[0])\n",
        "print(\"Example of encoded target: \", y_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ocam_c9kRj9B"
      },
      "source": [
        "timestep = X_train[0].shape[0] # the length of a sequence (sentence, or time series, etc) is the timestep. It can vary, but we chose to pad to have a fixed length. Why ?\n",
        "n_features = 1 #each input on each timestep only is one number, so it is of size 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8Wyf0pSHIj9"
      },
      "source": [
        "###Question 1 \n",
        "* Qu'avez-vous constaté comme phénomènes dans les données ?\n",
        "* Que fait la fonction tokenize_and_encode ?\n",
        "* Pourquoi dans le code de tokenize_and_encode, utilise-t-on keras.preprocessing.pad_sequences ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXWN8mdBgnja"
      },
      "source": [
        "On définit pour aller plus vite dans l'exploration graphique des résulats deux fonction : plot_history et plot_confusion_matrix. Il suffira de les évoquer après l'entrainement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qZ1Rmptn0iC"
      },
      "source": [
        "#Fonction outil pour la suite\n",
        "def plot_history(history):\n",
        "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
        "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
        "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
        "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
        "    \n",
        "    if len(loss_list) == 0:\n",
        "        print('Loss is missing in history')\n",
        "        return \n",
        "    \n",
        "    ## As loss always exists\n",
        "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
        "    \n",
        "    ## Loss\n",
        "    plt.figure(1)\n",
        "    for l in loss_list:\n",
        "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
        "    for l in val_loss_list:\n",
        "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
        "    \n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    \n",
        "    ## Accuracy\n",
        "    plt.figure(2)\n",
        "    for l in acc_list:\n",
        "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
        "    for l in val_acc_list:    \n",
        "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
        "\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puFoj8fXoF0J"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        title='Normalized confusion matrix'\n",
        "    else:\n",
        "        title='Confusion matrix'\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "    \n",
        "## multiclass or binary report\n",
        "## If binary (sigmoid output), set binary parameter to True\n",
        "def full_multiclass_report(model,\n",
        "                           x,\n",
        "                           y_true,\n",
        "                           classes,\n",
        "                           batch_size=32,\n",
        "                           binary=False):\n",
        "    \n",
        "    # 2. Predict probabilities and stores in y_pred\n",
        "    y_pred = model.predict(x, batch_size=batch_size)\n",
        "    y_pred = y_pred.argmax(axis=1)\n",
        "    y_true = y_true.argmax(axis=1)\n",
        "    \n",
        "    # 3. Print accuracy score\n",
        "    print(\"Accuracy : \"+ str(accuracy_score(y_true,y_pred)))\n",
        "    \n",
        "    print(\"\")\n",
        "    \n",
        "    # 4. Print classification report\n",
        "    print(\"Classification Report\")\n",
        "    print(classification_report(y_true,y_pred,digits=5))    \n",
        "    \n",
        "    # 5. Plot confusion matrix\n",
        "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    print(cnf_matrix)\n",
        "    plot_confusion_matrix(cnf_matrix,classes=classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ8DLJTgQzsF"
      },
      "source": [
        "##3. Mon premier RNN\n",
        "\n",
        "Il s'agit ici de l'architecture de base d'un RNN comme vu précédemment.\n",
        "On ajoute une couche d'embedding, puis une couche SimpleRNN, qui est la couche RNN de base. Cette couche ne va retourner que le dernier état caché. Enfin, on ajoute une couche d'output complètement connectée.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvQEOG1nhpMP"
      },
      "source": [
        "epochs = 25\n",
        "batch_size = 16\n",
        "lr = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHDM0e-lPA9o"
      },
      "source": [
        "# Keras' input for any recurrent neural network requires the following shape : (batch_size, timestep, n_features)\n",
        "#batch_size can be None, it will be input at train time, this is a default in Keras if all timesteps are equal\n",
        "#timestep is the length of your sequence\n",
        "#n_features is the number of features on each timestep. Had you done some embedding pre-processing, it would be larger than 1\n",
        "input=Input(shape=(timestep))\n",
        "embedding = Embedding(input_dim=vocab_size, output_dim=200, mask_zero=True, embeddings_initializer=\"zeros\")(input)\n",
        "rnn_layer = SimpleRNN(units = 24, activation = \"sigmoid\", recurrent_initializer= \"random_normal\")(embedding) #in : size (None, timesteps, 64), out : size (None, units)\n",
        "dense_layer=Dense(2, activation = \"softmax\")(rnn_layer) #in : size(None, units), out : size(None,1)\n",
        "model_rnn=Model(input,dense_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4F1_d4Bh0bo"
      },
      "source": [
        "On compile le modèle avec un SGD, stochastic gradient descend, qui est lui aussi la version de base pour les stratégies d'optimisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hKcSh19U5g4"
      },
      "source": [
        "#Output the model skeleton, and compile\n",
        "model_rnn.summary()\n",
        "\n",
        "optim = SGD(learning_rate=lr)\n",
        "model_rnn.compile(optimizer=optim,loss=\"binary_crossentropy\", metrics = [\"binary_accuracy\"])\n",
        "\n",
        "#print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(model.layers[1].get_weights())) #affiche les poids de la couche n*1, cad la couche SimpleRNN\n",
        "history = History()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEdcp6o7WRPE"
      },
      "source": [
        "history=model_rnn.fit(X_train, y_train, epochs =epochs, batch_size=batch_size, verbose = 1, validation_split=0.2, callbacks = [history])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pju3msJ1iFLw"
      },
      "source": [
        "On regarde les résulats, notamment les courbes de loss et d'accuracy, ainsi que la matrice de confusion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUWsmK7jjssp"
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H6PGdOPoNa3"
      },
      "source": [
        "full_multiclass_report(model_rnn, X_test, y_test, [0,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwgO0MTZ8vVx"
      },
      "source": [
        "### Question 2\n",
        "\n",
        "*    A quoi sert la couche d'embedding ?\n",
        "*    Quand considérez-vous qu'il y a du sur-apprentissage (overfitting) ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgLEDXLVWnb0"
      },
      "source": [
        "### Question 3: \n",
        "C'est beau comme modèle  ! Maintenant, enlevez de votre couche d'embedding le paramètre `mask=True`. Oh-oh...\n",
        "*    Quel comportment de votre RNN pendant l'entrainement vous met la puce à l'oreille que quelque chose ne va pas ? \n",
        "\n",
        "Décommenter la ligne `#print_weights`, et relancer l'entrainement avec le callback print_weights.\n",
        "\n",
        "Ce problème est un problème courant des réseaux de neurones récurrents (mais pas que !), il s'appelle le <font color='red'>Vanishing Gradient</font>, ou la disparition du gradient. Plus de détails ici :\n",
        "https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484\n",
        "\n",
        "*   Pouvez-vous expliquer simplement le phénomène ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZajxdW0G1ks"
      },
      "source": [
        "##4. LSTM, évolution du RNN\n",
        "\n",
        "Des solutions au gradient qui disparait sont:\n",
        "  - changer de fonction d'activation, sigmoid est bien identifié pour poser des problèmes. (pourquoi ?)\n",
        "  - bien initialiser les poids (pourquoi ?)\n",
        "  - ajouter des couches de normalisation (pourquoi ?)\n",
        "  - avec tensorflow, masquer proprement (nous venons de le voir)\n",
        "Dans le cas inverse, un gradient qui explose, souvent, on clippe le gradient.\n",
        "\n",
        "Le LSTM résout plusieurs problèmes liées aux RNN de base, mais surtout celui du gradient qui disparait ou explose. Il permet aussi plus de finesse dans ce dont on se \"souvient\" ou pas dans le réseau.\n",
        "\n",
        "---\n",
        "\n",
        "Nous allons reprendre notre problème initial de classification de messages, mais maintenant avec un LSTM, et des fonctions d'activation plus fines.\n",
        "\n",
        "Pour le LSTM, la documentation Keras est bien faite :\n",
        "https://keras.io/api/layers/recurrent_layers/lstm/ et \n",
        "https://keras.io/api/layers/activations/\n",
        "\n",
        "---\n",
        "\n",
        "Attention, le choix par défaut est quand même un choix ! Soyez au moins conscients d'avoir choisi telle fonction d'activation ou telle initialisation, car nous avons vu que cela peut avoir de grosses conséquences. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3pHQ5qRkHL8"
      },
      "source": [
        "#hyperparametrees\n",
        "epochs = 20\n",
        "batch_size = 128\n",
        "lr = 0.04"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQFBbFafVleX"
      },
      "source": [
        "#architecture du modele\n",
        "input=Input(shape=(timestep))\n",
        "embedding = Embedding(input_dim=vocab_size, output_dim=200, mask_zero=True)(input)\n",
        "lstm_layer = LSTM(units = 64, recurrent_activation = \"relu\")(embedding)\n",
        "dense_layer = Dense(2, activation = \"softmax\")(lstm_layer)\n",
        "\n",
        "model_lstm=Model(input,dense_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDdi627wkX7x"
      },
      "source": [
        "On choisit ici un optimiseur un peu différent : Adam. Il est plus adapté à des opération complexes, en partie car il permet au learning rate de s'adapter. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mq03w21WBpM"
      },
      "source": [
        "#compilation\n",
        "from keras.utils import plot_model\n",
        "plot_model(model_lstm)\n",
        "\n",
        "optim = Adam(learning_rate=lr)\n",
        "model_lstm.compile(optimizer=optim,loss=\"binary_crossentropy\", metrics = [\"binary_accuracy\"])\n",
        "\n",
        "history = History()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HliccGPkYeIs"
      },
      "source": [
        "history=model_lstm.fit(X_train, y_train, epochs =epochs, batch_size=batch_size, verbose = 1, validation_split=0.2, callbacks = [history])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNwTqSmxn2Wv"
      },
      "source": [
        "plot_history(history)\n",
        "full_multiclass_report(model_lstm, X_test, y_test, [0,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrbSqloRkquG"
      },
      "source": [
        "###Question 5\n",
        "\n",
        "Retirez `mask=True`  la couche d'embedding pour ce modèle. Y a-t-il toujours des problèmes ? Arrivent-ils toujours aussi vite ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDewXT39UkAU"
      },
      "source": [
        "##5. Attention\n",
        "\n",
        "Cette attention est un many-to-one, prenant donc les outputs d'un LSTM et applicant l'attention pour obtenir seulement le dernier output, car nous faisons de la classification. Pour de la traduction par exemple, la couche serait différente.\n",
        "\n",
        "Code de l'attention inspiré de ce blog :\n",
        "https://matthewmcateer.me/blog/getting-started-with-attention-for-classification/\n",
        "et de ce repository GitHub :\n",
        "https://github.com/philipperemy/keras-attention-mechanism/blob/master/attention/attention.py\n",
        "\n",
        "\n",
        "Keras possède aussi un layer d'Attention selon Luong et d'attention selon Badanau. https://keras.io/api/layers/attention_layers/additive_attention/. Nous ne l'utilisons pas pour voir que les couches Keras sont des opérations mathématiques à part entière, et qu'on peut faire toutes les opérations décrites dans les équations précédentes avec quelques couches Keras. A retenir : savoir à quoi correspondent les couches et ce que l'on met dans son modèle est plus important que d'empiler des couches aveuglément et d'avoir plein de puissance de calcul."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8fDmgwdof2G"
      },
      "source": [
        "#hyperparametres\n",
        "epochs = 20\n",
        "batch_size = 128\n",
        "lr = 0.04"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycmfEHVCGTGO"
      },
      "source": [
        "#Modèle avec attention type Luong\n",
        "\n",
        "input=Input(shape=(timestep))\n",
        "embedding = Embedding(input_dim=vocab_size, output_dim=200, mask_zero=True)(input)\n",
        "(lstm_seq, h_t, state_c) = LSTM(units = 128, recurrent_activation = \"relu\", return_sequences=True, return_state=True)(embedding)\n",
        "\n",
        "#on calcule le score pour h_t\n",
        "units = int(h_t.shape[1])\n",
        "score_first_part = Dense(units, name=\"attention_score_part_1\")(lstm_seq) #that is the W*h_s_bar in the slides' equations\n",
        "score =  dot([score_first_part, h_t], [2, 1], name='attention_score') # that is the h_t*W*h_s_bar in the slides' equations\n",
        "\n",
        "#on calcule les poids d'attention\n",
        "attention_weights = Activation(\"softmax\", name=\"attention_weights\")(score)\n",
        "  \n",
        "#le vecteur de contexte est le produit scalaire des vecteur d'attention et des états cachés du LSTM\n",
        "context_vector = dot([attention_weights,lstm_seq], [1, 1], name =\"context_vector\")\n",
        "\n",
        "#on crée le vecteur d'attention en concetenant context et h_t, et en l'activant pat tanh\n",
        "concat_att_state= concatenate([context_vector, h_t])\n",
        "attention_vector = Dense(64, use_bias=False, activation='tanh', name='attention_vector')(concat_att_state)\n",
        "\n",
        "dense_layer = Dense(2, activation = \"relu\", name=\"output\")(attention_vector)\n",
        "model_att = Model(inputs=input, outputs=dense_layer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5xRGLJ4mQC8"
      },
      "source": [
        "Nous utilisons la version fonctionelle de Keras plutot que sa version séquentielle, ce que vous avez du remarquer dans ce TP. \n",
        "Cela permet d'aller chercher individuellement les couches et leurs outputs, de recycler des outputs, et de faire plusieurs outputs pour un même modèle.\n",
        "Cela permet donc de pouvoir utiliser `lstm_seq`, qui n'est autre que la séquence $\\bar{h}_1, \\bar{h}_2,...,\\bar{h}_S$, autant de fois que nous avons besoin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXfU0LhQNJQ-"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model_att)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXqK-kBtNMVn"
      },
      "source": [
        "model_att.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIczMFnDl3Cx"
      },
      "source": [
        "Nous allons visulaiser l'attention. Pour cela, il nous faut obtenir les poids d'attention. \n",
        "Puisque nous utilisons la version fonctionelle de Keras, nous pouvons réaliser un petit tour de passe-passe. Nous créons un modèle `a_w_model` pour attention weight model, que nous initialisons avec les mêmes couches que le modèle `model_att`. Ce n'est pas seulement les mêmes noms, ce sont les mêmes couches avec les mêmes matrices de poids, partagées entre les deux modèles ! \n",
        "\n",
        "De cette façon, entrainer `model_att` va faire varier identiquement les poids pour `a_w_model`. L'output de `a_w_model` étant la layer \"attention_weights\" de `model_att`, il suffit de prédire quelque chose avec `a_w_model` pour obtenir les poids d'attention.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeIGdZ4I5U37"
      },
      "source": [
        "#model for attention weights, share the exact same layers as our main model\n",
        "a_w_model = Model(inputs= model_att.inputs, outputs = model_att.get_layer(\"attention_weights\").output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QOOI7fqoSPy"
      },
      "source": [
        "Nous définissons un callback, pour pouvoir visualiser les poids d'attention à la fin de chaque époque et voir comment ils varient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boT0oCAZw2QZ"
      },
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "# to visualise, we use the great boutny of keras' functional model.\n",
        "#we create our model that gets trained. And then on the side, we create another model, that shares exactly the same layers, but stops at the attention layer\n",
        "#the first model will train, and because they share layers, the layers of the visu model will also train\n",
        "#it suffices to get the output of the visu model to get the attention weights of the trained model\n",
        "\n",
        "class VisualiseAttentionMap(Callback):\n",
        "    def __init__(self, attention_weights_model, x_test, max_epochs, output_dir=None):\n",
        "        super(Callback, self).__init__()\n",
        "        self.visu_model = attention_weights_model\n",
        "        # best_weights to store the weights at which the minimum loss occurs.\n",
        "        self.x_test = x_test\n",
        "        self.output_dir = output_dir\n",
        "        self.max_epoch = max_epochs\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        attention_map = self.visu_model.predict(self.x_test)\n",
        "        #print(attetion_map.shape)\n",
        "        # top is attention map.\n",
        "        # bottom is ground truth.\n",
        "        #plt.imshow(np.concatenate([attention_map, x_test_mask]), cmap='hot')\n",
        "\n",
        "        plt.imshow(attention_map, cmap='hot')\n",
        "\n",
        "        iteration_no = str(epoch).zfill(3)\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Iteration {iteration_no} / {self.max_epoch}')\n",
        "        if self.output_dir is not None :\n",
        "          if not os.path.exists(self.output_dir):\n",
        "            os.makedirs(self.output_dir)\n",
        "          plt.savefig(f'{output_dir}/epoch_{iteration_no}.png')\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLZak6fHotpv"
      },
      "source": [
        "On compile, on définit les callbacks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krNnOPR2ot7b"
      },
      "source": [
        "optim = Adam(learning_rate =lr, clipnorm=1)\n",
        "model_att.compile(optimizer=optim,loss=\"binary_crossentropy\", metrics = [\"binary_accuracy\"])\n",
        "\n",
        "history = History()\n",
        "#earlyStop = EarlyStopping(monitor = \"val_binary_accuracy\", patience = 4, restore_best_weights=True)\n",
        "attmap = VisualiseAttentionMap(a_w_model, X_train[0:10], epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lURVZE8u5H2"
      },
      "source": [
        "model_att.fit(X_train, y_train, epochs =epochs, batch_size=128, verbose = 1, validation_split=0.2, \n",
        "              callbacks =[history, attmap])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-UTD7sg3S58"
      },
      "source": [
        "plot_history(history)\n",
        "full_multiclass_report(model_att, X_test, y_test, [0,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PGsNMpNoz1O"
      },
      "source": [
        "### Question 6\n",
        "\n",
        "Faites tourner le modèle avec attention sur 20 époques. \n",
        "*   Que constatez-vous au niveau des résultats et de la rapidité de convergence ? Est-ce qu'on sur-apprend ?\n",
        "\n",
        "Rajouter l’early stopping dans le modèle.\n",
        "\n",
        "*   Qu’a permis de faire l’attention dans ce modèle ? \n",
        "\n",
        "*   Que permet de faire le Callback que nous avons défini, VisualAttentionMap ? Qu’observez-vous au niveau des outputs de VisualAttentionMap ? \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZDZGRElW1Te"
      },
      "source": [
        "###Question 7\n",
        "\n",
        "A votre avis, l'attention est-elle nécéssaire dans notre cas de figure, avec spam-ou-ham ? Si oui, pourquoi, si non, pourquoi pas ?"
      ]
    }
  ]
}