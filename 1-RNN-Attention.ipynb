{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "introLstmAttention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOjPSOGVMCGnL16l/qKkLNS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CMallart/ateliers-NN/blob/main/1-RNN-Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTuuSwd9xG4_"
      },
      "source": [
        "# RNN et compagnie\n",
        "\n",
        "Nous allons voir dans ce notrebook ce qu'est un RNN, et plus précisément un LSTM, une intuition de pourquoi et dans quels cas il fonctionne, ainsi que ses limitations. Et avec ça, nous allons traduire des choses !\n",
        "\n",
        "Puis nous allons ajouter un petit plus qui change beaucoup de choses : un mécanisme d'attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdzCjRdSx_K-"
      },
      "source": [
        "# Mon premier LSTM\n",
        "\n",
        "Théorie plus un petit exemple jouet de classification de topic dans des phrases.\n",
        "\n",
        "Objectifs :\n",
        "- Comprendre la théorie derrière un RNN, et une LSTM\n",
        "- Importer et nettoyer des données textuelles, les encoder\n",
        "- L'importance du choix des activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bafOuN9ypkg"
      },
      "source": [
        "## Le pourquoi du comment\n",
        "\n"
      ]
    }
  ]
}