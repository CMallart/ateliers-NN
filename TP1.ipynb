{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CMallart/ateliers-NN/blob/main/TP1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxDxQxGGjxcE"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TrIDLwh4RT4"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYKA8fbikJUa"
      },
      "source": [
        "# usefull functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XivNo8Tf4IQF"
      },
      "source": [
        "def one_hot_encode_database(database, list_id):\n",
        "  \"\"\"One hot encode the columns of the database specified in list_id\n",
        "\n",
        "  return a copy of the database, on hot encoded\n",
        "  \"\"\"\n",
        "  encoded_database = np.empty(shape=(database.shape[0], 0), dtype=float)\n",
        "  for id in range(database.shape[1]):\n",
        "      if id in list_id:\n",
        "          original_column = database[:, id]\n",
        "          encoded_column = to_categorical(original_column.astype(int), num_classes=int(np.max(original_column)+1))\n",
        "          encoded_database = np.column_stack((encoded_database, encoded_column))\n",
        "      else:\n",
        "          original_column = database[:, id]\n",
        "          encoded_database = np.column_stack((encoded_database, original_column))\n",
        "  return encoded_database"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKsMRYfOk3dw"
      },
      "source": [
        "def normalize_database(database, list_id):\n",
        "  \"\"\"Normalize between 0 and 1 each column of the database specified in list_id.\n",
        "\n",
        "  return a copy of the database, normalized. \n",
        "  \"\"\"\n",
        "  encoded_database = database.copy()\n",
        "  for id in list_id:\n",
        "      encoded_database[:, id] = (encoded_database[:, id] - np.amin(encoded_database[:, id])) / (np.amax(encoded_database[:, id]) - np.amin(encoded_database[:, id]))\n",
        "  return encoded_database"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL2YKwFuk_e7"
      },
      "source": [
        "def build_NN(layer_list, input_dim, output_dim, lr=0.001):\n",
        "  \"\"\"Construct a layer composed of dense layers, which dimensions are definded in the layer_list argument.\n",
        "\n",
        "  return the constructed and compiled model.\n",
        "  \"\"\"\n",
        "  # Q7: add layers and \"compile\" the model\n",
        "  model = Sequential()\n",
        "  #Add layers\n",
        "  model.add()\n",
        "    \n",
        "  #Compile the network\n",
        "  model.compile(loss=None, optimizer=None, metrics=None)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoHbcM8Pleny"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2OomsrSlj9u"
      },
      "source": [
        "## Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRxqrKRl0dqA"
      },
      "source": [
        "nb_fold = 5\n",
        "layer_list = [50, 50, 20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiCpRfVj0pNI"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE2osmbv0pUc"
      },
      "source": [
        "rng = np.random.default_rng()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24J5BLeTxSqI"
      },
      "source": [
        "# Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6KLdlKKxSyf",
        "outputId": "2075348f-6cfe-4af9-b491-636f8243a318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "\n",
        "database = load_wine()\n",
        "print('--- original database ---\\n')\n",
        "print(f'{database.data.shape[0]} examples, {database.data.shape[1]} features')\n",
        "for i in [10, 80, 140]:\n",
        "  print(f'label of example {i:3d}: {database.target[i]}')\n",
        "\n",
        "# Q7: one hot encode the class and normalize the features of the database here\n",
        "\n",
        "random_indices = np.arange(data.shape[0])\n",
        "rng.shuffle(random_indices)\n",
        "data = data[random_indices,:]\n",
        "target = target[random_indices,:]\n",
        "print('\\n--- after randomization ---\\n')\n",
        "print(f'{data.shape[0]} examples, {data.shape[1]} features')\n",
        "for i in [10, 80, 140]:\n",
        "  print(f'label of example {i:3d}: {target[i]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- original database ---\n",
            "\n",
            "178 examples, 13 features\n",
            "label of example  10: 0\n",
            "label of example  80: 1\n",
            "label of example 140: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-855aa09e2af8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Q7: one hot encode the class and normalize the features of the database here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrandom_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-eK49yzlzqA"
      },
      "source": [
        "## Build / Fit / avaluate (quick check) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnabyC7Ylz85"
      },
      "source": [
        "# Q10: quick check\n",
        "model = build_NN(layer_list, data.shape[1], target.shape[1], lr=0.1)\n",
        "model.summary()\n",
        "model.fit(data, target, batch_size=16, epochs=100)\n",
        "model.evaluate(data, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYUDQqyAl5S7"
      },
      "source": [
        "## Build / Fit / avaluate (cross-validated) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP9BNryyl5gL"
      },
      "source": [
        "gt_and_pred = np.empty(shape=(2, 0))\n",
        "\n",
        "for train_index, test_index in KFold(n_splits=nb_fold).split(data):\n",
        "  training_data, validation_data = data[train_index], data[test_index]\n",
        "  training_target, validation_target = target[train_index], target[test_index]\n",
        "\n",
        "  # Q11: test several learning rates\n",
        "  model = build_NN(layer_list, data.shape[1], target.shape[1], lr=0.1)\n",
        "\n",
        "  callback = EarlyStopping(patience=5)\n",
        "  # Q13: add EarlyStopping() callback\n",
        "  model.fit(training_data, training_target, batch_size=16, epochs=200, validation_data=(validation_data, validation_target), shuffle=True)\n",
        "\n",
        "  fold_prediction = model.predict(validation_data)\n",
        "  gt_and_pred = np.column_stack((gt_and_pred, np.array([np.argmax(validation_target, axis=1), np.argmax(fold_prediction, axis=1)])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPob7bUl6JFY"
      },
      "source": [
        "# Q12: display confusion matrix and balanced accuracy here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}